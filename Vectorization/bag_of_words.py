from nltk.tokenize import *
import pandas as pd
import nltk
import numpy as np
import os
from tqdm import tqdm
"""
This program is a bag of words text vectorizer that takes a csv of text data 
"""

def write_corpus(IS_WINDOWS=False):
    """
    Write corpus is only for first time use as it writes the complete review corpus to a txt file
    """
    if IS_WINDOWS:
        dataframe = pd.read_csv('C:/Users/nicho/OneDrive/Desktop/Project/Data/normalized_dataset_new.csv')
    else:
        dataframe = pd.read_csv('/Users/nick/Desktop/Current Classes/PSTAT 131/Project/Data/normalized_dataset_new.csv')

    list_of_all_reviews = dataframe['Review Text'].tolist()
    final_text = ""
    
    line = tqdm(desc="Writing Reviews into txt", total=len(list_of_all_reviews))
    for review in list_of_all_reviews:
        if type(review) != float:
            final_text += review
        line.update(1)
    line.close()



    if IS_WINDOWS:
        with open('C:/Users/nicho/OneDrive/Desktop/Project/Data/dataset_corpus.txt', 'w',encoding='utf-8') as f:
            f.write(final_text)
    else:
        with open("/Users/nick/Desktop/dataset_corpus.txt", 'w', encoding='utf-8') as f:
            f.write(final_text)

def create_word_distribution(frequency_size, IS_WINDOWS=False):
    """
    Reads in an entire corpus and creates an NLKT frequency distribution for it 
    :return: list of the most common words within a corpus
    :rtype: [(string, int)]
    """
    if IS_WINDOWS:
        with open('C:/Users/nicho/OneDrive/Desktop/Project/Data/dataset_corpus.txt', 'r') as f:
            text = f.read()
    else:
        with open("/Users/nick/Desktop/Current Classes/PSTAT 131/Project/Data/dataset_corpus.txt", 'r') as f:
            text = f.read()    
            
    tokens = word_tokenize(text)
    distribution = nltk.FreqDist(tokens)
    return distribution.most_common(frequency_size)

def get_vocab(distribution_list):
    """
    Gets the vocab for a body of text based off of a frequency distribution made by NLTK and writes it onto
    a file for later use
    :param distribution_list: a list of tuples generated by nltk with the most frequent words in a dataset
    :type distribution_list: [(string, int)]
    :return: None
    :rtype: None
    """
    vocabulary = {}
    other = {}
    i = 0
    for x, y in distribution_list:
        vocabulary[x] = i
        other[x] = y
        i += 1
    return vocabulary, other
    
def create_vector(vocab_positional_dictionary, text, frequency_size):
    """
    Takes a review and vectorizes it for bag of words vectorization method
    :param vocab_positional_dictionary: A dictionary for vector positions
    :type vocab_positional_dictionary: dict
    :param text: A review text for processing
    :type text: 
    """
    try:
        review_tokens = word_tokenize(text)
        base_array = np.zeros((frequency_size,), dtype=int)
        review_vector = list(base_array)
        for word in review_tokens:
            if word in vocab_positional_dictionary:
                review_vector[vocab_positional_dictionary[word]] += 1
        return review_vector
    except:
        print(f"the following review was providing an error: {text}")
        return None

def create_vocab_dictionary(path):
    """
    Creates a vocab dictionary for use within the vectorizer
    :return: A dictionary with most common words and their positions on a vector
    :rtype: {word:position}
    """
    with open(path, 'r') as f:
        dict_as_string = f.read()

    return eval(dict_as_string)


